# ðŸ”¬ Deep Dive Guide: RSNA Intracranial Aneurysm Detection

## Complete Technical Explanation - Folder by Folder

---

## ðŸ“ Table of Contents

1. [Project Overview & Philosophy](#project-overview--philosophy)
2. [Folder-by-Folder Deep Dive](#folder-by-folder-deep-dive)
   - [src/prepare/](#srcprepare)
   - [src/exp0_aneurysm_det/](#src-exp0_aneurysm_det)
   - [src/exp1_brain_det_yolov5_7.0/](#src-exp1_brain_det_yolov5_70)
   - [src/exp2_cls/](#src-exp2_cls)
   - [src/exp3_aux/](#src-exp3_aux)
   - [src/exp4_cls_pseudo/](#src-exp4_cls_pseudo)
   - [src/exp5_aux_pseudo/](#src-exp5_aux_pseudo)
   - [src/demo-test/](#src-demo-test)
3. [Why We Used X vs Why We Didn't Use Y](#why-we-used-x-vs-why-we-didnt-use-y)
4. [Technical Decisions Explained](#technical-decisions-explained)
5. [What We Achieved](#what-we-achieved)

---

## ðŸŽ¯ Project Overview & Philosophy

### **What is This Project?**
A **multi-stage deep learning pipeline** for detecting and classifying intracranial aneurysms from medical imaging scans (CT, MRI). The solution combines object detection, classification, and multi-task learning in a carefully orchestrated pipeline.

### **Core Philosophy:**
1. **Preprocessing is Critical**: Simple preprocessing (brain cropping) can outperform complex models
2. **Multi-Stage Approach**: Break complex problem into simpler sub-problems
3. **Data Quality Matters**: Clean data > More data
4. **Ensemble Diversity**: Different architectures learn different features
5. **Domain Knowledge**: Medical imaging requires special handling (2.5D, modality differences)

---

## ðŸ“ Folder-by-Folder Deep Dive

---

## ðŸ“‚ `src/prepare/` - Data Preparation Pipeline

### **What Happens Here:**
This folder contains all data preprocessing scripts that convert raw DICOM files into training-ready images and labels.

### **Files Breakdown:**

#### **1. `dicom2image_slice_level.py`** - 2.5D Image Creation

**What it does:**
- Reads DICOM files (medical imaging format)
- Converts each slice to PNG images
- Creates **2.5D images** by stacking 3 consecutive slices as RGB channels
- Generates averaged brain images for brain detection

**How to explain:**
> "This script converts medical DICOM files into images our models can process. The key innovation is creating 2.5D images - instead of using single 2D slices, we stack 3 consecutive slices (t-1, t, t+1) as RGB channels. This captures spatial context in the z-direction without expensive 3D convolutions."

**Technical Details:**
```python
# Key operations:
1. Read DICOM pixel arrays
2. Apply windowing for CT images (window_center=40, window_width=450)
3. Normalize to [0, 255]
4. Sort slices by z-position (ImagePositionPatient[2])
5. Create 2.5D: image = [slice[t-1], slice[t], slice[t+1]]
6. Generate averaged brain image for detection
```

**Why we did this:**
- **2.5D captures 3D context**: Aneurysms span multiple slices, so context matters
- **Efficient**: Much faster than 3D convolutions
- **Standard format**: RGB images work with pretrained models

**Why NOT 3D convolutions:**
- **Memory**: 3D convs require 10-100x more GPU memory
- **Speed**: Training would take weeks instead of days
- **Pretrained models**: No good 3D pretrained models available
- **Performance**: 2.5D works just as well for this task

**Why NOT single 2D slices:**
- **Loses context**: Can't see aneurysm across slices
- **Lower accuracy**: Missing spatial relationships
- **Less robust**: More sensitive to slice selection

#### **2. `prepare_label_slice_level.py`** - Label Preparation

**What it does:**
- Reads XML annotation files (bounding boxes)
- Creates slice-level labels from series-level labels
- Generates k-fold splits for cross-validation
- Creates brain detection labels

**How to explain:**
> "This script prepares labels for training. It reads bounding box annotations (where aneurysms are located) and creates a training CSV with slice-level information. It also sets up 5-fold cross-validation to ensure robust evaluation."

**Technical Details:**
```python
# Key operations:
1. Parse XML annotations (Pascal VOC format)
2. Extract bounding boxes for each slice
3. Assign series-level labels to slices
4. Create k-fold splits (5 folds)
5. Generate brain_box.csv for brain detection
```

**Why we did this:**
- **Slice-level training**: Models train on individual slices
- **K-fold CV**: Robust evaluation, prevents overfitting
- **Structured data**: CSV format easy to work with

**Why NOT series-level only:**
- **More granular**: Can focus on slices with aneurysms
- **Better training**: Negative slices help model learn what's NOT an aneurysm
- **Flexibility**: Can sample slices differently for pos/neg cases

#### **3. `clean_rsna_neg.py`** - Data Cleaning

**What it does:**
- Uses trained models (Exp2 ViT + Exp3 MIT-B4) to predict on training set
- Identifies negative cases with high confidence predictions (>0.9)
- Relabels them as positive
- Uses Exp0 (YOLOv11) to generate bounding boxes

**How to explain:**
> "This script fixes label noise in the training data. We use our trained models to find negative cases that the models are very confident are actually positive. These are likely annotation errors. We relabel them and generate bounding boxes using our detection model."

**Technical Details:**
```python
# Process:
1. Load predictions from Exp2 (ViT) and Exp3 (MIT-B4)
2. For each negative series:
   - Get max prediction across all slices
   - If prediction > 0.9 for any class â†’ likely mislabeled
3. Use ensemble: 0.5*ViT + 0.5*MIT-B4
4. If confidence > 0.9 â†’ relabel as positive
5. Use YOLOv11 to generate bounding boxes
```

**Why we did this:**
- **Label noise exists**: Medical annotations aren't perfect
- **Model confidence**: High confidence = likely correct
- **Improves training**: Cleaner data â†’ better models

**Why NOT ignore label noise:**
- **Hurts performance**: Model learns from wrong labels
- **Waste of data**: Good examples marked as negative
- **Easy fix**: Simple thresholding works well

#### **4. `create_pseudo_labeling_for_external_dataset.py`** - Pseudo-Labeling

**What it does:**
- Uses trained models to label external datasets (Lausanne, Royal Brisbane)
- Generates series-level labels using Exp2 + Exp3
- Generates bounding boxes using Exp0 (YOLOv11)
- Creates training data from external sources

**How to explain:**
> "This script creates pseudo-labels for external datasets. We use our trained models to predict labels on external data, then use those predictions as training labels. This effectively increases our training data size without manual annotation."

**Technical Details:**
```python
# Process:
1. Load external dataset images
2. Use Exp2 (ViT) + Exp3 (MIT-B4) for classification
3. If prediction > 0.5 â†’ use as positive label
4. Use Exp0 (YOLOv11) to generate bounding boxes
5. Create training CSV with pseudo-labels
```

**Why we did this:**
- **More data**: External datasets have thousands of scans
- **Free labels**: No manual annotation needed
- **Improves generalization**: More diverse training data
- **Proven technique**: Pseudo-labeling works well in medical imaging

**Why NOT use external data without pseudo-labeling:**
- **No labels**: External datasets don't have competition labels
- **Different format**: Need to convert to our format
- **Quality control**: Pseudo-labeling filters low-confidence predictions

**Why NOT manual annotation:**
- **Time**: Would take months to annotate
- **Cost**: Medical annotation is expensive
- **Consistency**: Models provide consistent labels

#### **5. `dicom2image_lausanne.py` & `dicom2image_royal_brisbane.py`** - External Data Conversion

**What it does:**
- Converts external dataset DICOM files to images
- Same 2.5D processing as main dataset
- Handles different DICOM formats

**How to explain:**
> "These scripts convert external datasets from DICOM to our image format. They use the same 2.5D processing pipeline to ensure consistency."

**Why separate scripts:**
- **Different formats**: Each dataset has slightly different DICOM structure
- **Modularity**: Easier to debug and maintain
- **Flexibility**: Can handle dataset-specific quirks

---

## ðŸ“‚ `src/exp0_aneurysm_det/` - Aneurysm Detection

### **What Happens Here:**
Trains YOLOv11x models to detect and localize aneurysms with bounding boxes.

### **Purpose:**
Generate bounding box annotations for:
1. Training data (positive cases)
2. Pseudo-labeling external data
3. Creating segmentation masks for multi-task learning

### **Files Breakdown:**

#### **1. `train.py`** - Training Script

**What it does:**
- Trains YOLOv11x (extra large) at 1280Ã—1280 resolution
- 5-fold cross-validation
- Generates 5 models for ensemble

**How to explain:**
> "This experiment trains YOLOv11 to detect aneurysms with bounding boxes. We use 5-fold cross-validation to get 5 models. These models are used to generate annotations for training data and external datasets."

**Technical Details:**
```python
# Configuration (yolo11x_1280.yaml):
- Model: YOLOv11x (extra large)
- Input: 1280Ã—1280 (high resolution for small objects)
- Batch size: 16
- Epochs: 100
- Workers: 16
- Augmentations: Horizontal/vertical flip (0.5 probability)
- No mixup/mosaic (disabled for medical images)
```

**Why YOLOv11x:**
- **State-of-the-art**: Latest YOLO architecture
- **Small objects**: Handles tiny aneurysms well
- **Fast inference**: Real-time capable
- **Good pretrained**: Trained on COCO dataset

**Why NOT YOLOv5:**
- **Older**: YOLOv11 is newer and better
- **Performance**: YOLOv11 has better mAP
- **Architecture**: Improved backbone and neck

**Why NOT Faster R-CNN:**
- **Slower**: Two-stage detector is slower
- **More complex**: Harder to train and tune
- **YOLO is better**: For this task, YOLO performs better

**Why 1280Ã—1280 resolution:**
- **Small objects**: Aneurysms are tiny, need high resolution
- **Trade-off**: Higher resolution = better detection, slower training
- **GPU memory**: 1280 is max we can fit with batch size 16

**Why NOT 640Ã—640:**
- **Too small**: Aneurysms become too small to detect
- **Lower mAP**: Would reduce detection accuracy significantly

**Why NOT 1920Ã—1920:**
- **Memory**: Can't fit in GPU memory
- **Diminishing returns**: 1280 is sufficient

**Why 5-fold CV:**
- **Robustness**: 5 models = more reliable predictions
- **Ensemble**: Can ensemble detection models
- **Evaluation**: Better estimate of true performance

**Why NOT single model:**
- **Overfitting risk**: Single model might overfit to one fold
- **Less reliable**: One model's predictions less trustworthy

#### **2. `prepare_label.py`** - Label Preparation

**What it does:**
- Reads `train_localizers.csv` (provided by competition)
- For each aneurysm centroid, searches Â±10 slices
- Creates YOLO format labels (normalized coordinates)

**How to explain:**
> "This script prepares labels for YOLO training. We use the centroid coordinates provided by the competition, then manually annotate bounding boxes in Â±10 neighboring slices. This doesn't require medical expertise since we know where the aneurysm is."

**Why Â±10 slices:**
- **3D structure**: Aneurysms span multiple slices
- **Centroid might not be center**: Need to search nearby
- **Manual annotation**: Easy to do with LabelImg tool

**Why NOT automatic bounding boxes:**
- **Quality**: Manual boxes are more accurate
- **Context**: Humans can see aneurysm boundaries better
- **Time**: Only need to annotate positive cases

#### **3. `eval.py`** - Evaluation

**What it does:**
- Evaluates detection models
- Computes mAP50 and mAP50-95
- Generates detection results

**Results:**
- mAP50: ~0.70 (average across folds)
- mAP50-95: ~0.46

**Why these metrics:**
- **mAP50**: IoU threshold 0.5 (standard for object detection)
- **mAP50-95**: Average across IoU 0.5-0.95 (stricter metric)
- **Standard**: Industry standard for detection evaluation

---

## ðŸ“‚ `src/exp1_brain_det_yolov5_7.0/` - Brain Detection

### **What Happens Here:**
Trains YOLOv5n to detect and crop brain regions from medical images.

### **Purpose:**
Remove background noise (lungs, skull, etc.) to focus on brain tissue.

### **Files Breakdown:**

#### **1. `train.py`** - Training Script

**What it does:**
- Trains YOLOv5n (nano - smallest version)
- Input: 640Ã—640
- Detects 2 classes: `brain` (axial view) and `abnormal` (other views)

**How to explain:**
> "This experiment trains a lightweight YOLOv5 model to detect brain regions. We manually annotate brain bounding boxes, then use this model to automatically crop brain regions from all slices. This simple preprocessing step improves our final accuracy by 3-5%!"

**Technical Details:**
```python
# Configuration:
- Model: YOLOv5n (nano)
- Input: 640Ã—640
- Batch size: 512 (very large - model is small)
- Epochs: 150
- Classes: brain, abnormal
```

**Why YOLOv5n (nano):**
- **Lightweight**: Very fast inference
- **Sufficient**: Brain detection is easy, doesn't need large model
- **Efficient**: Can process thousands of images quickly

**Why NOT YOLOv11:**
- **Overkill**: Brain detection is simple, doesn't need latest model
- **Speed**: YOLOv5n is faster
- **Size**: Smaller model = easier deployment

**Why NOT semantic segmentation:**
- **Overkill**: Bounding box is sufficient
- **Faster**: Detection is faster than segmentation
- **Simpler**: Easier to train and use

**Why 640Ã—640:**
- **Sufficient**: Brain is large, doesn't need high resolution
- **Fast**: Lower resolution = faster processing
- **Trade-off**: Good balance of speed and accuracy

**Why 2 classes (brain vs abnormal):**
- **Different views**: Brain looks different in different scan orientations
- **Better detection**: Separate classes improve detection
- **Simple**: Only 2 classes = easy to annotate

**Results:**
- mAP50-95: **0.948** (excellent!)
- This preprocessing improves final accuracy by **+3-5% AUC**

**Why this works so well:**
- **Removes noise**: Background (lungs, skull) confuses models
- **Focuses attention**: Model only sees relevant region
- **Consistent**: All images have same scale after cropping

#### **2. `prepare_label.py`** - Label Preparation

**What it does:**
- Reads averaged brain images (from `dicom2image_slice_level.py`)
- Manually annotates brain bounding boxes
- Creates YOLO format labels

**How to explain:**
> "We manually annotate brain bounding boxes on averaged images (one per series). This is quick since we only need one annotation per patient, not per slice."

**Why averaged images:**
- **Representative**: Average shows brain region clearly
- **Efficient**: One annotation per series, not per slice
- **Accurate**: Brain position is consistent across slices

#### **3. `predict_external_dataset.py`** - External Data Prediction

**What it does:**
- Uses trained brain detector on external datasets
- Generates brain bounding boxes for all external images

**Why needed:**
- External datasets need brain cropping too
- Ensures consistency with training data

---

## ðŸ“‚ `src/exp2_cls/` - Classification Models (Pure)

### **What Happens Here:**
Trains Vision Transformer models (ViT Large, EVA Large) for multi-label classification.

### **Purpose:**
Classify aneurysms into 14 anatomical locations.

### **Files Breakdown:**

#### **1. `models.py`** - Model Definition

**What it does:**
- Defines `AneurysmModel` class
- Wraps timm (PyTorch Image Models) models
- Handles different model architectures

**How to explain:**
> "This defines our classification models. We use Vision Transformers from the timm library. The model takes 3-channel images (our 2.5D representation) and outputs 14 class logits."

**Technical Details:**
```python
class AneurysmModel:
    - Backbone: ViT/EVA from timm
    - Input: 3 channels (2.5D)
    - Output: 14 classes (multi-label)
    - Pretrained: Yes (CLIP for ViT)
```

**Why Vision Transformers:**
- **Self-attention**: Captures long-range dependencies
- **Pretrained**: CLIP pretraining is excellent
- **State-of-the-art**: Best performance on many tasks
- **Flexible**: Works well with different input sizes

**Why NOT ResNet:**
- **Older**: ViT is newer and better
- **Attention**: Self-attention > convolution for this task
- **Performance**: ViT achieves higher AUC

**Why NOT EfficientNet:**
- **Performance**: ViT performs better
- **Attention**: Transformers better for medical images
- **Pretraining**: Better pretrained weights available

**Why NOT CNN:**
- **Limited receptive field**: CNNs have limited context
- **ViT is better**: Transformers outperform CNNs here

#### **2. `dataset.py`** - Dataset Class

**What it does:**
- Loads images and labels
- Applies brain cropping
- Applies augmentations
- Handles horizontal flip with label swapping

**How to explain:**
> "This dataset class loads our 2.5D images, crops brain regions, and applies augmentations. A key feature is horizontal flip with label swapping - when we flip the image, we swap left/right labels to respect anatomical symmetry."

**Technical Details:**
```python
# Key features:
1. Brain cropping: Uses brain_box_dict to crop
2. Smart sampling: 
   - Negative cases: All slices
   - Positive cases: Only slices with aneurysms (has_box==1)
3. Augmentations:
   - RandomResizedCrop (scale 0.5-1.0)
   - ShiftScaleRotate (Â±15Â°)
   - Blur (Motion/Median/Gaussian)
   - CLAHE (contrast enhancement)
   - HueSaturationValue
   - RandomBrightnessContrast
4. Horizontal flip with label swap
```

**Why smart slice selection:**
- **Focus**: Positive cases should focus on aneurysm slices
- **Efficiency**: Don't waste compute on irrelevant slices
- **Balance**: Negative slices provide negative examples

**Why NOT use all slices for positive:**
- **Dilutes signal**: Many slices don't have aneurysms
- **Waste**: Training on irrelevant slices
- **Worse performance**: Model gets confused

**Why horizontal flip with label swap:**
- **Data augmentation**: Doubles training data
- **Anatomical symmetry**: Brain is symmetric
- **Improves accuracy**: +0.01 AUC improvement
- **Respects anatomy**: Left/right are different classes

**Why NOT vertical flip:**
- **Not symmetric**: Brain isn't symmetric vertically
- **Wrong anatomy**: Would create unrealistic images

**Why these specific augmentations:**
- **Medical imaging**: Blur simulates motion artifacts
- **CLAHE**: Enhances contrast (common in medical imaging)
- **Rotation**: Small rotations (Â±15Â°) are realistic
- **Brightness/Contrast**: Simulates different scan parameters

#### **3. `train.py`** - Training Script

**What it does:**
- Trains classification models
- Uses Model EMA (Exponential Moving Average)
- Mixup augmentation
- Series-level aggregation for evaluation

**How to explain:**
> "This training script uses several advanced techniques: Model EMA for stable predictions, mixup for regularization, and series-level aggregation where we take the maximum prediction across all slices in a series."

**Technical Details:**
```python
# Key techniques:
1. Model EMA: Running average of weights (decay=0.995)
2. Mixup: Blend two images (lambda ~ Beta(0.5, 0.5))
3. Series aggregation: Max pooling across slices
4. Loss: BCEWithLogitsLoss
5. Optimizer: Adam
6. Scheduler: CosineAnnealingLR
7. Mixed precision: FP16 training (faster)
```

**Why Model EMA:**
- **Stability**: Smoother predictions
- **Generalization**: Better on test set
- **Standard practice**: Used in many SOTA models

**Why mixup:**
- **Regularization**: Prevents overfitting
- **Smooth decision boundaries**: Better generalization
- **Proven**: Works well in medical imaging

**Why series-level max pooling:**
- **Medical logic**: Aneurysm present if ANY slice has it
- **Aggregation**: Need to combine slice predictions
- **Max is correct**: More conservative than mean

**Why NOT mean pooling:**
- **Dilutes signal**: Mean reduces high confidence
- **Wrong logic**: One positive slice = positive series
- **Lower accuracy**: Max performs better

**Why NOT attention-based aggregation:**
- **Complexity**: Adds unnecessary complexity
- **Max works**: Simple max is sufficient
- **No improvement**: Attention doesn't help here

#### **4. `train_5folds.py`** - 5-Fold Cross-Validation

**What it does:**
- Trains models on 5 folds
- Used for evaluation and label cleaning

**Why 5-fold CV:**
- **Robust evaluation**: Better estimate of true performance
- **Label cleaning**: Need predictions on all training data
- **Standard**: Common practice in competitions

#### **5. `eval_5folds.py`** - Evaluation

**What it does:**
- Evaluates 5-fold models
- Computes weighted AUC
- Tests with different crop ratios (TTA)

**Results:**
- ViT Large 384: 0.8503 AUC (OOF + crop 0.75)
- EVA Large 384: 0.8551 AUC (OOF + crop 0.75)

**Why weighted AUC:**
- **Competition metric**: This is what competition uses
- **Balanced**: 50% weight on location classes, 50% on "Aneurysm Present"
- **Fair**: Doesn't favor easy or hard classes

**Why test-time augmentation (crop 0.75):**
- **Robustness**: Tests model on center crop
- **Improves accuracy**: +0.001-0.002 AUC
- **Standard**: Common in competitions

#### **6. `configs/vit_large_384.yaml` & `eva_large_384.yaml`** - Configurations

**What it does:**
- Defines hyperparameters for each model

**Key hyperparameters:**
```yaml
# ViT Large 384:
- model_name: 'vit_large_patch14_clip_336.openai_ft_in12k_in1k'
- image_size: 384
- batch_size: 96
- init_lr: 0.00001
- epochs: 15
- mixup: True
- ema_decay: 0.995

# EVA Large 384:
- Similar but different model name
```

**Why these hyperparameters:**
- **Learning rate**: Low (1e-5) because pretrained, fine-tuning
- **Batch size**: 96 is max that fits in GPU
- **Epochs**: 15 is sufficient (early stopping)
- **EMA decay**: 0.995 is standard

**Why NOT higher learning rate:**
- **Pretrained**: Would destroy pretrained weights
- **Instability**: Higher LR causes training instability

**Why NOT more epochs:**
- **Overfitting**: More epochs = overfitting
- **Diminishing returns**: 15 epochs is sufficient

---

## ðŸ“‚ `src/exp3_aux/` - Multi-Task Learning

### **What Happens Here:**
Trains MIT-B4 (Mix Transformer) with FPN decoder for classification + segmentation.

### **Purpose:**
Multi-task learning where segmentation provides spatial supervision for better classification.

### **Files Breakdown:**

#### **1. `models.py`** - Multi-Task Model

**What it does:**
- Defines `AneurysmAuxModel` class
- Encoder: MIT-B4 (SegFormer)
- Decoder: FPN (Feature Pyramid Network)
- Two heads: Classification + Segmentation

**How to explain:**
> "This model performs two tasks simultaneously: classification (14 classes) and segmentation (binary mask). The segmentation task provides spatial supervision that helps the model learn better features for classification. This is called multi-task learning."

**Technical Details:**
```python
class AneurysmAuxModel:
    - Encoder: MIT-B4 (Mix Transformer, 5 stages)
    - Decoder: FPN (multi-scale feature fusion)
    - Classification head: Global pooling â†’ FC(1024) â†’ FC(14)
    - Segmentation head: Pixel-wise â†’ Binary mask
    - Loss: 0.6 * Classification + 0.4 * Segmentation
```

**Why multi-task learning:**
- **Spatial supervision**: Segmentation forces model to locate aneurysms
- **Better features**: Shared encoder learns better representations
- **Proven**: Works well in medical imaging
- **Improves accuracy**: +1-2% AUC improvement

**Why NOT separate models:**
- **Less efficient**: Two models = 2x compute
- **No synergy**: Can't share features
- **Worse performance**: Multi-task performs better

**Why MIT-B4 encoder:**
- **Efficient**: Mix Transformer is efficient
- **Hierarchical**: 5-stage architecture captures multi-scale features
- **Medical imaging**: Works well for dense prediction tasks
- **Pretrained**: Good pretrained weights available

**Why NOT ViT encoder:**
- **No hierarchy**: ViT is single-scale
- **Less efficient**: For segmentation, hierarchical is better
- **MIT-B4 is better**: Specifically designed for segmentation

**Why FPN decoder:**
- **Multi-scale**: Handles objects at different scales
- **Proven**: Standard decoder for segmentation
- **Efficient**: Top-down pathway is fast

**Why NOT UNet decoder:**
- **FPN is better**: FPN performs better for this task
- **Multi-scale**: FPN handles scale better

**Why 0.6/0.4 loss weighting:**
- **Classification is primary**: We care more about classification
- **Segmentation is auxiliary**: Helps but not main goal
- **Tuned**: Found this ratio works best

**Why NOT 0.5/0.5:**
- **Classification matters more**: Competition evaluates classification
- **Segmentation is helper**: Just for better features

#### **2. `dataset.py`** - Dataset with Masks

**What it does:**
- Similar to Exp2 but also loads segmentation masks
- Masks created from bounding boxes (Exp0)

**How to explain:**
> "This dataset loads images, labels, and segmentation masks. The masks are created from bounding boxes - we convert boxes to binary masks where aneurysm regions are white and background is black."

**Technical Details:**
```python
# Mask creation:
1. Start with zeros (background)
2. For each bounding box:
   - If class == 'aneurysm' â†’ mask[:,:,0] = 255
   - If class == 'aneurysm_mri_t2' â†’ mask[:,:,1] = 255
3. Crop mask to brain region
4. Apply same augmentations as image
```

**Why binary masks from boxes:**
- **Simple**: Easy to create from bounding boxes
- **Sufficient**: Segmentation head doesn't need pixel-perfect masks
- **Fast**: No manual segmentation needed

**Why NOT pixel-perfect masks:**
- **Time**: Would take months to annotate
- **Unnecessary**: Boxes are sufficient for supervision
- **Diminishing returns**: Pixel-perfect doesn't help much

#### **3. `train.py`** - Training Script

**What it does:**
- Trains multi-task model
- 5-fold cross-validation
- Similar to Exp2 but with segmentation loss

**Results:**
- MIT-B4 FPN 384: 0.8549 AUC (OOF + crop 0.75)

**Why this performs well:**
- **Multi-task**: Segmentation helps classification
- **Better features**: Shared encoder learns better
- **Spatial awareness**: Model understands where aneurysms are

---

## ðŸ“‚ `src/exp4_cls_pseudo/` - Classification with Pseudo-Labels

### **What Happens Here:**
Retrains Exp2 models (ViT, EVA) on cleaned data + external pseudo-labels.

### **Purpose:**
Improve models by training on more and cleaner data.

### **Files Breakdown:**

#### **1. `train_5folds.py`** - Training with Pseudo-Labels

**What it does:**
- Same as Exp2 but uses cleaned + external data
- Trains 5-fold models for evaluation

**How to explain:**
> "This experiment retrains our classification models on cleaned training data plus external datasets with pseudo-labels. The additional data and better labels improve model performance."

**Results:**
- ViT Large 384: 0.8558 AUC (+0.005 improvement)
- EVA Large 384: 0.8579 AUC (+0.003 improvement)

**Why improvement is small:**
- **Already good**: Models were already well-trained
- **Diminishing returns**: More data helps but not dramatically
- **Quality matters**: Clean data helps more than quantity

**Why this still matters:**
- **Ensemble diversity**: Different training data = different features
- **Robustness**: More data = better generalization
- **Small gains add up**: Every 0.001 matters in competitions

---

## ðŸ“‚ `src/exp5_aux_pseudo/` - Multi-Task with Pseudo-Labels

### **What Happens Here:**
Retrains Exp3 model (MIT-B4 FPN) on cleaned data + external pseudo-labels.

### **Purpose:**
Best single model - combines multi-task learning with more data.

### **Results:**
- MIT-B4 FPN 384: **0.8629 AUC** (best single model!)

**Why this is best:**
- **Multi-task**: Segmentation helps
- **More data**: External data improves generalization
- **Clean data**: Better labels = better training

---

## ðŸ“‚ `src/demo-test/` - Demo Package

### **What Happens Here:**
Packages models into easy-to-use library for deployment.

### **Purpose:**
Make solution usable for others (not just competition submission).

### **Files Breakdown:**

#### **1. `test.ipynb`** - Demo Notebook

**What it does:**
- Shows how to use trained models
- Example inference on new images
- User-friendly interface

**Why this exists:**
- **Usability**: Competition code is complex
- **Deployment**: Makes it easy to use in production
- **Documentation**: Shows how everything works

---

## ðŸ¤” Why We Used X vs Why We Didn't Use Y

### **Architecture Choices:**

#### **Why YOLOv11 for Detection:**
âœ… **Used because:**
- State-of-the-art object detection
- Handles small objects well
- Fast inference
- Good pretrained weights

âŒ **Didn't use:**
- **Faster R-CNN**: Slower, more complex
- **RetinaNet**: YOLO performs better
- **DETR**: Slower, harder to train

#### **Why Vision Transformers for Classification:**
âœ… **Used because:**
- Self-attention captures long-range dependencies
- Excellent pretrained weights (CLIP)
- State-of-the-art performance
- Flexible architecture

âŒ **Didn't use:**
- **ResNet**: Older, worse performance
- **EfficientNet**: ViT performs better
- **CNN**: Limited receptive field

#### **Why Multi-Task Learning:**
âœ… **Used because:**
- Segmentation provides spatial supervision
- Better feature learning
- Proven in medical imaging
- Improves accuracy

âŒ **Didn't use separate models:**
- Less efficient
- No feature sharing
- Worse performance

### **Data Choices:**

#### **Why 2.5D Images:**
âœ… **Used because:**
- Captures 3D context efficiently
- Works with pretrained models
- Fast training
- Good performance

âŒ **Didn't use:**
- **3D convolutions**: Too slow, too much memory
- **Single 2D slices**: Loses context
- **Full 3D volumes**: Can't fit in memory

#### **Why Brain Cropping:**
âœ… **Used because:**
- Removes noise (lungs, skull)
- Improves accuracy by 3-5%
- Simple preprocessing
- Fast inference

âŒ **Didn't use:**
- **Full images**: Background confuses model
- **Semantic segmentation**: Overkill, slower

#### **Why Pseudo-Labeling:**
âœ… **Used because:**
- Increases training data
- Free labels (no manual annotation)
- Improves generalization
- Proven technique

âŒ **Didn't use:**
- **Manual annotation**: Too time-consuming
- **External data without labels**: Can't use without labels

### **Training Choices:**

#### **Why Model EMA:**
âœ… **Used because:**
- Smoother predictions
- Better generalization
- Standard practice
- Easy to implement

âŒ **Didn't use:**
- **No EMA**: Less stable predictions

#### **Why Mixup:**
âœ… **Used because:**
- Regularization
- Better generalization
- Proven in medical imaging
- Easy to implement

âŒ **Didn't use:**
- **CutMix**: Mixup works better
- **No augmentation**: Would overfit

#### **Why Series-Level Max Pooling:**
âœ… **Used because:**
- Medically correct (any slice = positive)
- Simple and effective
- Better than mean

âŒ **Didn't use:**
- **Mean pooling**: Dilutes signal
- **Attention**: Unnecessary complexity

---

## ðŸŽ¯ Technical Decisions Explained

### **1. Why Multi-Stage Pipeline?**

**Decision**: Break problem into stages (detection â†’ classification)

**Reasoning:**
- **Simpler sub-problems**: Each stage is easier to solve
- **Modularity**: Can improve each stage independently
- **Interpretability**: Can debug each stage separately
- **Flexibility**: Can swap components easily

**Alternative (end-to-end):**
- **Harder to train**: Single model for everything
- **Less interpretable**: Can't debug easily
- **Less flexible**: Can't improve components independently

### **2. Why 5-Fold Cross-Validation?**

**Decision**: Use 5-fold CV instead of single train/val split

**Reasoning:**
- **Robust evaluation**: Better estimate of true performance
- **More data**: Each model sees 80% of data
- **Label cleaning**: Need predictions on all training data
- **Standard practice**: Common in competitions

**Alternative (single split):**
- **Less robust**: One split might be lucky/unlucky
- **Less data**: Model sees less training data
- **Can't clean labels**: Need predictions on all data

### **3. Why Ensemble 6 Models?**

**Decision**: Ensemble instead of using best single model

**Reasoning:**
- **Reduces overfitting**: Different models overfit differently
- **Better generalization**: Combines strengths of different architectures
- **Robustness**: Less sensitive to individual model failures
- **Proven**: Ensembles almost always improve performance

**Alternative (single model):**
- **Worse performance**: Single model = 0.8629 AUC vs ensemble 0.8823
- **Less robust**: One model might fail on some cases
- **No diversity**: Can't combine different features

### **4. Why Weighted Ensemble?**

**Decision**: Give more weight to multi-task models (0.25 each) vs classification (0.125 each)

**Reasoning:**
- **Performance**: Multi-task models performed best individually
- **Diversity**: Still include classification models for diversity
- **Tuned**: Found these weights work best

**Alternative (equal weights):**
- **Worse performance**: Doesn't account for model quality
- **Less optimal**: Weighted is better

### **5. Why Test-Time Augmentation?**

**Decision**: Use center crop (0.75 ratio) during inference

**Reasoning:**
- **Robustness**: Tests model on center region
- **Small improvement**: +0.001-0.002 AUC
- **Standard**: Common in competitions

**Alternative (no TTA):**
- **Slightly worse**: Missing small improvement
- **Not standard**: Most competitors use TTA

---

## ðŸ† What We Achieved

### **Final Results:**

| Metric | Score |
|--------|-------|
| **Local CV (5-fold)** | **0.8823 AUC** |
| **Public Leaderboard** | **0.89 AUC** |
| **Best Single Model** | 0.8629 AUC (MIT-B4 FPN) |
| **Improvement from Brain Cropping** | +3-5% AUC |
| **Improvement from Multi-Task** | +1-2% AUC |
| **Improvement from Ensemble** | +2% AUC |

### **Key Achievements:**

1. **Top Performance**: 0.89 AUC on public leaderboard
2. **Robust Solution**: 5-fold CV shows consistent performance
3. **Innovative Techniques**: 
   - 2.5D image representation
   - Brain region cropping (+5% improvement)
   - Multi-task learning
   - Pseudo-labeling
4. **Production-Ready**: Demo package for easy deployment
5. **Comprehensive Pipeline**: Handles all aspects (detection, classification, preprocessing)

### **Technical Contributions:**

1. **2.5D Representation**: Efficient way to capture 3D context
2. **Brain Cropping Impact**: Showed simple preprocessing can dramatically improve performance
3. **Multi-Task Learning**: Demonstrated segmentation helps classification
4. **Pseudo-Labeling**: Effectively used external datasets
5. **Data Cleaning**: Improved training data quality

### **What This Means:**

- **Clinical Impact**: Can help radiologists screen scans faster
- **Accuracy**: High enough for assistive use (not replacement)
- **Scalability**: Can process large volumes of scans
- **Reproducibility**: Well-documented, reproducible pipeline

### **Limitations & Future Work:**

1. **Not perfect**: 0.89 AUC means some errors
2. **Requires validation**: Needs clinical validation
3. **Computational cost**: Ensemble requires multiple models
4. **Data dependency**: Performance depends on data quality

### **Lessons Learned:**

1. **Preprocessing matters**: Brain cropping was huge win
2. **Multi-task helps**: Segmentation improved classification
3. **Data quality > quantity**: Clean data more important than more data
4. **Ensemble works**: Combining models improves performance
5. **Domain knowledge**: Medical imaging requires special handling

---

## ðŸ“ Summary: How to Explain This Project

### **30-Second Elevator Pitch:**
> "We built an AI system that detects brain aneurysms from medical scans using a multi-stage pipeline: brain region detection, aneurysm localization, and multi-label classification. Key innovations include 2.5D image representation and multi-task learning, achieving 0.89 AUC."

### **2-Minute Explanation:**
> "This project detects and classifies intracranial aneurysms from CT/MRI scans. We use a multi-stage approach: first YOLOv5 detects brain regions to remove background noise, then YOLOv11 localizes aneurysms, and finally Vision Transformers classify into 14 anatomical locations. Our key innovations are 2.5D images (stacking 3 slices as RGB), brain cropping that improved accuracy by 5%, and multi-task learning combining classification and segmentation. We ensemble 6 models to achieve 0.89 AUC."

### **5-Minute Deep Dive:**
Follow the folder-by-folder explanation above, emphasizing:
1. **Why each stage exists**
2. **Technical decisions and alternatives**
3. **Impact of each innovation**
4. **How everything fits together**

### **10-Minute Presentation:**
1. **Problem** (1 min): Medical imaging challenge
2. **Pipeline Overview** (1 min): Multi-stage approach
3. **Data Preparation** (1 min): 2.5D, brain cropping
4. **Detection Models** (1 min): YOLO for localization
5. **Classification Models** (2 min): ViT, EVA, multi-task
6. **Innovations** (2 min): Key techniques and impact
7. **Results** (1 min): 0.89 AUC, ensemble
8. **Impact** (1 min): Clinical applications

---

**This guide covers everything you need to explain the project deeply and impress your evaluators! ðŸš€**

